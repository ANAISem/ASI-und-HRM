# HRM-Modell in Trae nutzen - Komplette Anleitung

## ğŸ¯ Ziel
Das HRM-Modell als lokales KI-Modell in Trae verwenden und aus der Modell-Liste auswÃ¤hlen.

## ğŸ“‹ Schnellstart

### 1. Installation ausfÃ¼hren
```bash
# Im Terminal ausfÃ¼hren:
./install_hrm_in_trae.sh
```

### 2. In Trae registrieren
1. Ã–ffne Trae
2. Gehe zu **Settings** â†’ **Extensions** â†’ **MCP Servers**
3. Klicke auf **Add Server**
4. Konfiguriere:
   - **Name**: `HRM-Local`
   - **Type**: `command`
   - **Command**: `/Users/bigsur/Desktop/HRM - Hirarchisches Reasoning Model/venv/bin/python`
   - **Args**: `/Users/bigsur/Desktop/HRM - Hirarchisches Reasoning Model/mcp_hrm_server.py`

## ğŸ”§ Manuelle Installation (falls benÃ¶tigt)

### Vorbereitung
```bash
# Virtuelle Umgebung erstellen
python3 -m venv venv
source venv/bin/activate

# AbhÃ¤ngigkeiten installieren
pip install -r requirements_mcp.txt
```

### Server starten
```bash
# MCP-Server starten
python mcp_hrm_server.py
```

## ğŸ® Verwendung in Trae

### Als Modell auswÃ¤hlen
Nach erfolgreicher Registrierung:
1. Ã–ffne eine neue Konversation
2. In der Modell-Auswahl: **HRM-Local** auswÃ¤hlen
3. Oder verwende: `@HRM-Local` in der Eingabe

### Verwendungsbeispiele
```
# Direkte Verwendung
@HRM-Local Analysiere diesen Python-Code

# Als Standardmodell setzen
Settings â†’ AI â†’ Default Model â†’ HRM-Local

# FÃ¼r spezielle Aufgaben
@HRM-Local FÃ¼hre hierarchisches Reasoning fÃ¼r diese Funktion durch
```

## ğŸ—ï¸ Technische Details

### Modell-Parameter
- **Parameter**: 105 Millionen
- **GrÃ¶ÃŸe**: ~404 MB RAM
- **Typ**: Hierarchical Reasoning Model
- **Lokal**: 100% CPU-basiert

### MCP-Endpunkte
- **Tool**: `hrm_reasoning`
- **Funktion**: Code-Analyse und Reasoning
- **Privacy**: Keine Cloud-Kommunikation

### Beispiel-API-Aufruf
```python
# Beispiel fÃ¼r direkte Nutzung
import requests

response = requests.post('http://localhost:8000/reason', json={
    'prompt': 'Analysiere diese Funktion',
    'code': 'def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)'
})
```

## ğŸ› ï¸ Fehlerbehebung

### Server startet nicht
```bash
# ÃœberprÃ¼fe Python-Pfad
which python
# Sollte sein: /Users/bigsur/Desktop/HRM - Hirarchisches Reasoning Model/venv/bin/python

# ÃœberprÃ¼fe Dateipfade
ls -la mcp_hrm_server.py
```

### Trae erkennt Server nicht
1. **Neustart**: Trae komplett neu starten
2. **Pfad prÃ¼fen**: Absolute Pfade verwenden
3. **Berechtigungen**: `chmod +x` fÃ¼r Skripte

### Performance-Optimierung
```bash
# FÃ¼r schnelleren Start
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
```

## ğŸ“Š Monitoring

### Server-Status prÃ¼fen
```bash
# Health-Check
curl -X POST http://localhost:8000/health

# Logs anzeigen
tail -f server.log
```

### Ressourcen-Ãœberwachung
- **CPU**: `htop` oder Activity Monitor
- **RAM**: `free -h` oder Activity Monitor
- **GPU**: Nicht verwendet (CPU-only)

## ğŸ”„ Updates

### Server aktualisieren
```bash
# Neue Version installieren
git pull origin main
pip install -r requirements_mcp.txt --upgrade
```

### Konfiguration anpassen
```bash
# Server-Konfiguration bearbeiten
nano mcp_hrm_server.py
```

## ğŸ†˜ Support

### HÃ¤ufige Fragen
**Q: Warum erscheint HRM-Local nicht in der Liste?**
A: ÃœberprÃ¼fe die MCP-Server-Einstellungen und starte Trae neu.

**Q: Kann ich HRM als Standardmodell setzen?**
A: Ja, in Settings â†’ AI â†’ Default Model â†’ HRM-Local auswÃ¤hlen.

**Q: Ist meine Daten sicher?**
A: Ja, 100% lokale Verarbeitung, keine Cloud-Kommunikation.

### Debug-Modus
```bash
# Mit Debug-Logging starten
DEBUG=1 python mcp_hrm_server.py
```

---

**âœ… Fertig!** Das HRM-Modell ist nun als lokales Modell in Trae verfÃ¼gbar.