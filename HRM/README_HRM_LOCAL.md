# ðŸ§  HRM Lokales Reasoning-System

**Sofort einsatzbereites, datenschutzfreundliches Reasoning-System**

## âš¡ Schnellstart (30 Sekunden)

```bash
cd "/Users/bigsur/Desktop/HRM - Hirarchisches Reasoning Model"
bash start_hrm_server.sh
```

Server lÃ¤uft dann auf: http://localhost:8080

## ðŸ”§ Manuelle Installation

```bash
# Falls Skript nicht funktioniert:
cd "/Users/bigsur/Desktop/HRM - Hirarchisches Reasoning Model"
source ../HRM-Complete/hrm_env_py311/bin/activate
pip install flask torch requests
python hrm_reasoning_server.py
```

## ðŸ“¡ API-Endpunkte

### 1. Allgemeines Reasoning
```bash
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{"prompt": "ErklÃ¤re Vorteile von hierarchischem Reasoning", "context": "KI-Architektur"}'
```

### 2. Code-Analyse
```bash
curl -X POST http://localhost:8080/analyze \
  -H "Content-Type: application/json" \
  -d '{"code": "def fib(n): return n if n<=1 else fib(n-1)+fib(n-2)", "language": "python"}'
```

### 3. Health-Check
```bash
curl http://localhost:8080/health
```

### 4. Test-Endpunkt
```bash
curl http://localhost:8080/test
```

## ðŸ Python-Client Beispiel

```python
import requests

# Reasoning-Anfrage
response = requests.post('http://localhost:8080/reasoning', json={
    'prompt': 'Wie optimiere ich Algorithmen?',
    'context': 'Python-Optimierung'
})

result = response.json()
print(f"Reasoning: {result['reasoning']}")
print(f"Konfidenz: {result['confidence']}")
```

## ðŸŽ¯ Verwendungsbeispiele

### FÃ¼r Code-Reviews (lokal & privat):
```bash
curl -X POST http://localhost:8080/analyze \
  -d '{"code": "Ihr Python-Code hier", "language": "python"}'
```

### FÃ¼r logische Probleme:
```bash
curl -X POST http://localhost:8080/reasoning \
  -d '{"prompt": "ErklÃ¤re QuickSort", "context": "Algorithmen"}'
```

## ðŸ›¡ï¸ Datenschutz-Features

- âœ… **100% lokal** - Keine Cloud-Kommunikation
- âœ… **CPU-only** - Keine GPU nÃ¶tig
- âœ… **105M Parameter** - Kompakt & effizient
- âœ… **Sofort verfÃ¼gbar** - Nach 5 Sekunden Start

## ðŸ” Testen

```bash
python test_hrm_client.py
```

## ðŸ“Š Technische Details

- **Modell:** HierarchicalReasoningModel_ACTV1
- **Parameter:** 105,990,146
- **GrÃ¶ÃŸe:** ~404 MB RAM
- **Startzeit:** < 5 Sekunden
- **API-Format:** JSON REST

## ðŸš¨ Fehlerbehebung

**Server startet nicht?**
```bash
# PrÃ¼fe Python-Umgebung
source ../HRM-Complete/hrm_env_py311/bin/activate
python -c "import torch; print('OK')"

# Manuelle Installation
pip install flask torch requests
```

**Port 8080 belegt?**
```bash
# Server auf anderem Port starten
python hrm_reasoning_server.py  # Standard: 8080
# oder Ã¤ndere in der Datei: app.run(port=8081)
```

## ðŸŽ‰ Fertig!

Dein HRM-System ist **sofort einsatzbereit** fÃ¼r:
- Lokales Reasoning
- Private Code-Analysen
- Datenschutz-kritische Aufgaben
- Schnelle Antworten ohne Cloud